{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3fc6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Financial Education Dataset Generator\n",
    "=====================================\n",
    "A multi-agent system for generating 20k high-quality educational financial Q&A pairs.\n",
    "Optimized for Google Colab with Hugging Face models and 4-bit quantization.\n",
    "\n",
    "Features:\n",
    "- Uses Mistral-7B-Instruct with 4-bit quantization (fits in Colab free tier)\n",
    "- Generates data in segments to avoid memory issues\n",
    "- Saves progress continuously to prevent data loss\n",
    "- Includes deduplication and quality checks\n",
    "- Outputs JSONL format ready for training\n",
    "\n",
    "Usage in Google Colab:\n",
    "1. Run all cells or execute: !python financial_education_generator.py\n",
    "2. Dataset will be saved to 'financial_education_dataset.jsonl'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590bebca",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "SECTION 1: INSTALLATION AND IMPORTS\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1417955c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a53546",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def install_dependencies():\n",
    "    \"\"\"Install required packages for Google Colab.\"\"\"\n",
    "    packages = [\n",
    "        \"transformers>=4.36.0\",\n",
    "        \"accelerate>=0.25.0\",\n",
    "        \"bitsandbytes>=0.41.0\",\n",
    "        \"torch>=2.0.0\",\n",
    "        \"sentence-transformers>=2.2.0\",\n",
    "        \"tqdm\",\n",
    "        \"jsonlines\"\n",
    "    ]\n",
    "    \n",
    "    print(\"üì¶ Installing dependencies...\")\n",
    "    for package in packages:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(f\"‚ö†Ô∏è Warning: Could not install {package}\")\n",
    "    print(\"‚úÖ Dependencies installed successfully!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e4bacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run installation\n",
    "install_dependencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657eae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now import all required libraries\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import hashlib\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540106f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a38912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd0b586",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Try importing sentence-transformers for deduplication\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer, util\n",
    "    EMBEDDINGS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    EMBEDDINGS_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è sentence-transformers not available. Using hash-based deduplication.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c341fd8e",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "SECTION 2: CONFIGURATION\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f84cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration for the dataset generator.\"\"\"\n",
    "    \n",
    "    # Model settings\n",
    "    model_name: str = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "    use_quantization: bool = True\n",
    "    max_new_tokens: int = 512\n",
    "    temperature: float = 0.7\n",
    "    top_p: float = 0.9\n",
    "    \n",
    "    # Dataset settings\n",
    "    target_dataset_size: int = 20000\n",
    "    batch_size: int = 10  # Questions per batch\n",
    "    save_interval: int = 50  # Save every N Q&A pairs\n",
    "    \n",
    "    # Output settings\n",
    "    output_file: str = \"financial_education_dataset.jsonl\"\n",
    "    checkpoint_file: str = \"generator_checkpoint.json\"\n",
    "    log_file: str = \"generator.log\"\n",
    "    \n",
    "    # Quality settings\n",
    "    min_answer_length: int = 100\n",
    "    max_retries: int = 3\n",
    "    similarity_threshold: float = 0.85  # For deduplication\n",
    "    \n",
    "    # Memory management\n",
    "    clear_cache_interval: int = 100  # Clear GPU cache every N generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203c878a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize config\n",
    "CONFIG = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e283db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(CONFIG.log_file),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b16429b",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "SECTION 3: FINANCIAL CURRICULUM STRUCTURE\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52ba15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINANCIAL_CURRICULUM = {\n",
    "    \"Personal Finance\": {\n",
    "        \"subtopics\": [\n",
    "            \"Budgeting Basics\", \"Emergency Funds\", \"Debt Management\",\n",
    "            \"Credit Scores\", \"Saving Strategies\", \"Insurance Fundamentals\",\n",
    "            \"Retirement Planning\", \"Tax Basics\", \"Financial Goal Setting\",\n",
    "            \"Net Worth Calculation\"\n",
    "        ],\n",
    "        \"difficulty_range\": [\"beginner\", \"intermediate\"]\n",
    "    },\n",
    "    \"Investing Fundamentals\": {\n",
    "        \"subtopics\": [\n",
    "            \"Stock Market Basics\", \"Bonds and Fixed Income\", \"Mutual Funds\",\n",
    "            \"ETFs\", \"Index Investing\", \"Diversification\", \"Risk vs Return\",\n",
    "            \"Dollar Cost Averaging\", \"Dividend Investing\", \"Value Investing\"\n",
    "        ],\n",
    "        \"difficulty_range\": [\"beginner\", \"intermediate\", \"advanced\"]\n",
    "    },\n",
    "    \"Advanced Investing\": {\n",
    "        \"subtopics\": [\n",
    "            \"Options Trading\", \"Futures and Derivatives\", \"Technical Analysis\",\n",
    "            \"Fundamental Analysis\", \"Portfolio Management\", \"Asset Allocation\",\n",
    "            \"Hedge Funds\", \"Private Equity\", \"Real Estate Investment\",\n",
    "            \"Alternative Investments\"\n",
    "        ],\n",
    "        \"difficulty_range\": [\"intermediate\", \"advanced\"]\n",
    "    },\n",
    "    \"Macroeconomics\": {\n",
    "        \"subtopics\": [\n",
    "            \"GDP and Economic Growth\", \"Inflation and Deflation\",\n",
    "            \"Interest Rates\", \"Monetary Policy\", \"Fiscal Policy\",\n",
    "            \"Business Cycles\", \"Unemployment\", \"International Trade\",\n",
    "            \"Exchange Rates\", \"Central Banking\"\n",
    "        ],\n",
    "        \"difficulty_range\": [\"intermediate\", \"advanced\"]\n",
    "    },\n",
    "    \"Corporate Finance\": {\n",
    "        \"subtopics\": [\n",
    "            \"Financial Statements\", \"Ratio Analysis\", \"Cash Flow Management\",\n",
    "            \"Capital Budgeting\", \"Cost of Capital\", \"Capital Structure\",\n",
    "            \"Mergers and Acquisitions\", \"IPOs\", \"Corporate Valuation\",\n",
    "            \"Working Capital Management\"\n",
    "        ],\n",
    "        \"difficulty_range\": [\"intermediate\", \"advanced\"]\n",
    "    },\n",
    "    \"Cryptocurrency and Blockchain\": {\n",
    "        \"subtopics\": [\n",
    "            \"Bitcoin Basics\", \"Blockchain Technology\", \"Altcoins\",\n",
    "            \"Crypto Wallets\", \"Decentralized Finance (DeFi)\",\n",
    "            \"NFTs\", \"Crypto Exchanges\", \"Mining\", \"Smart Contracts\",\n",
    "            \"Crypto Regulation\"\n",
    "        ],\n",
    "        \"difficulty_range\": [\"beginner\", \"intermediate\", \"advanced\"]\n",
    "    },\n",
    "    \"Behavioral Finance\": {\n",
    "        \"subtopics\": [\n",
    "            \"Cognitive Biases\", \"Emotional Investing\", \"Herd Mentality\",\n",
    "            \"Loss Aversion\", \"Overconfidence Bias\", \"Anchoring\",\n",
    "            \"Mental Accounting\", \"Market Psychology\", \"Investor Behavior\",\n",
    "            \"Decision Making Under Uncertainty\"\n",
    "        ],\n",
    "        \"difficulty_range\": [\"intermediate\", \"advanced\"]\n",
    "    },\n",
    "    \"Financial Markets\": {\n",
    "        \"subtopics\": [\n",
    "            \"Stock Exchanges\", \"Bond Markets\", \"Forex Markets\",\n",
    "            \"Commodity Markets\", \"Money Markets\", \"Market Participants\",\n",
    "            \"Market Efficiency\", \"Market Regulations\", \"Trading Mechanisms\",\n",
    "            \"Market Indices\"\n",
    "        ],\n",
    "        \"difficulty_range\": [\"beginner\", \"intermediate\", \"advanced\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e28f42",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "QUESTION_TYPES = [\n",
    "    \"definition\",      # \"What is X?\"\n",
    "    \"conceptual\",      # \"How does X work?\"\n",
    "    \"comparison\",      # \"What is the difference between X and Y?\"\n",
    "    \"example\",         # \"Can you give an example of X?\"\n",
    "    \"application\",     # \"How can I apply X in practice?\"\n",
    "    \"misconception\",   # \"What is a common misconception about X?\"\n",
    "    \"importance\",      # \"Why is X important?\"\n",
    "    \"calculation\",     # \"How do you calculate X?\"\n",
    "    \"strategy\",        # \"What strategies can be used for X?\"\n",
    "    \"risk\"            # \"What are the risks associated with X?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3610cb68",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "SECTION 4: DATA STRUCTURES\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bff8d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class QAPair:\n",
    "    \"\"\"Represents a Question-Answer pair.\"\"\"\n",
    "    id: str\n",
    "    topic: str\n",
    "    subtopic: str\n",
    "    question: str\n",
    "    answer: str\n",
    "    difficulty: str\n",
    "    question_type: str\n",
    "    review_status: str = \"pending\"\n",
    "    sources: List[str] = field(default_factory=list)\n",
    "    created_at: str = field(default_factory=lambda: datetime.now().isoformat())\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return asdict(self)\n",
    "    \n",
    "    def to_jsonl(self) -> str:\n",
    "        return json.dumps(self.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920444a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GenerationState:\n",
    "    \"\"\"Tracks the current state of generation.\"\"\"\n",
    "    total_generated: int = 0\n",
    "    total_verified: int = 0\n",
    "    total_rejected: int = 0\n",
    "    current_topic_idx: int = 0\n",
    "    current_subtopic_idx: int = 0\n",
    "    question_hashes: set = field(default_factory=set)\n",
    "    answer_hashes: set = field(default_factory=set)\n",
    "    \n",
    "    def save_checkpoint(self, filepath: str):\n",
    "        \"\"\"Save current state to checkpoint file.\"\"\"\n",
    "        state_dict = {\n",
    "            \"total_generated\": self.total_generated,\n",
    "            \"total_verified\": self.total_verified,\n",
    "            \"total_rejected\": self.total_rejected,\n",
    "            \"current_topic_idx\": self.current_topic_idx,\n",
    "            \"current_subtopic_idx\": self.current_subtopic_idx,\n",
    "            \"question_hashes\": list(self.question_hashes),\n",
    "            \"answer_hashes\": list(self.answer_hashes)\n",
    "        }\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(state_dict, f)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_checkpoint(cls, filepath: str) -> 'GenerationState':\n",
    "        \"\"\"Load state from checkpoint file.\"\"\"\n",
    "        if os.path.exists(filepath):\n",
    "            with open(filepath, 'r') as f:\n",
    "                state_dict = json.load(f)\n",
    "            state = cls(\n",
    "                total_generated=state_dict[\"total_generated\"],\n",
    "                total_verified=state_dict[\"total_verified\"],\n",
    "                total_rejected=state_dict[\"total_rejected\"],\n",
    "                current_topic_idx=state_dict[\"current_topic_idx\"],\n",
    "                current_subtopic_idx=state_dict[\"current_subtopic_idx\"],\n",
    "                question_hashes=set(state_dict[\"question_hashes\"]),\n",
    "                answer_hashes=set(state_dict[\"answer_hashes\"])\n",
    "            )\n",
    "            return state\n",
    "        return cls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9d6fa1",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "SECTION 5: MODEL LOADING\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454ff67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelManager:\n",
    "    \"\"\"Manages the Hugging Face model with quantization.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.pipe = None\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the model with 4-bit quantization.\"\"\"\n",
    "        logger.info(f\"üîÑ Loading model: {self.config.model_name}\")\n",
    "        logger.info(f\"üñ•Ô∏è Device: {self.device}\")\n",
    "        \n",
    "        if self.device == \"cuda\":\n",
    "            logger.info(f\"üìä GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "        \n",
    "        # Configure 4-bit quantization\n",
    "        if self.config.use_quantization and self.device == \"cuda\":\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_use_double_quant=True\n",
    "            )\n",
    "            logger.info(\"‚úÖ Using 4-bit quantization (NF4)\")\n",
    "        else:\n",
    "            quantization_config = None\n",
    "            logger.info(\"‚ÑπÔ∏è Running without quantization\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.config.model_name,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Load model\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.config.model_name,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=\"auto\" if self.device == \"cuda\" else None,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32\n",
    "        )\n",
    "        \n",
    "        # Create pipeline\n",
    "        self.pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_new_tokens=self.config.max_new_tokens,\n",
    "            temperature=self.config.temperature,\n",
    "            top_p=self.config.top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        logger.info(\"‚úÖ Model loaded successfully!\")\n",
    "        \n",
    "        if self.device == \"cuda\":\n",
    "            memory_used = torch.cuda.memory_allocated() / 1e9\n",
    "            logger.info(f\"üìä GPU Memory Used: {memory_used:.2f} GB\")\n",
    "    \n",
    "    def generate(self, prompt: str) -> str:\n",
    "        \"\"\"Generate text from prompt.\"\"\"\n",
    "        try:\n",
    "            # Format prompt for Mistral\n",
    "            formatted_prompt = f\"[INST] {prompt} [/INST]\"\n",
    "            \n",
    "            result = self.pipe(formatted_prompt)\n",
    "            generated_text = result[0][\"generated_text\"]\n",
    "            \n",
    "            # Extract only the response (after [/INST])\n",
    "            if \"[/INST]\" in generated_text:\n",
    "                response = generated_text.split(\"[/INST]\")[-1].strip()\n",
    "            else:\n",
    "                response = generated_text.strip()\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Generation error: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear GPU cache to prevent OOM.\"\"\"\n",
    "        if self.device == \"cuda\":\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f9e9b8",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "SECTION 6: AGENT IMPLEMENTATIONS\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd09cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAgent:\n",
    "    \"\"\"Base class for all agents.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_manager: ModelManager, config: Config):\n",
    "        self.model = model_manager\n",
    "        self.config = config\n",
    "    \n",
    "    def process(self, *args, **kwargs):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b179b25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicPlannerAgent(BaseAgent):\n",
    "    \"\"\"Generates curriculum structure (uses predefined structure for consistency).\"\"\"\n",
    "    \n",
    "    def process(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return the financial curriculum.\"\"\"\n",
    "        logger.info(\"üìö TopicPlanner: Using predefined financial curriculum\")\n",
    "        return FINANCIAL_CURRICULUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538751d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionWriterAgent(BaseAgent):\n",
    "    \"\"\"Generates diverse educational questions for each topic.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_manager: ModelManager, config: Config):\n",
    "        super().__init__(model_manager, config)\n",
    "        self.question_templates = {\n",
    "            \"definition\": [\n",
    "                \"What is {concept}?\",\n",
    "                \"Define {concept} in simple terms.\",\n",
    "                \"What does {concept} mean in finance?\"\n",
    "            ],\n",
    "            \"conceptual\": [\n",
    "                \"How does {concept} work?\",\n",
    "                \"Explain the mechanism behind {concept}.\",\n",
    "                \"What is the principle behind {concept}?\"\n",
    "            ],\n",
    "            \"comparison\": [\n",
    "                \"What is the difference between {concept} and {related_concept}?\",\n",
    "                \"How does {concept} compare to {related_concept}?\",\n",
    "                \"When should you choose {concept} over {related_concept}?\"\n",
    "            ],\n",
    "            \"example\": [\n",
    "                \"Can you give a real-world example of {concept}?\",\n",
    "                \"How would {concept} apply in a practical scenario?\",\n",
    "                \"What is a good illustration of {concept}?\"\n",
    "            ],\n",
    "            \"application\": [\n",
    "                \"How can I apply {concept} to my personal finances?\",\n",
    "                \"What are practical ways to use {concept}?\",\n",
    "                \"How do investors use {concept}?\"\n",
    "            ],\n",
    "            \"misconception\": [\n",
    "                \"What are common misconceptions about {concept}?\",\n",
    "                \"What do most people get wrong about {concept}?\",\n",
    "                \"What myths exist around {concept}?\"\n",
    "            ],\n",
    "            \"importance\": [\n",
    "                \"Why is {concept} important for investors?\",\n",
    "                \"What is the significance of {concept} in finance?\",\n",
    "                \"Why should someone learn about {concept}?\"\n",
    "            ],\n",
    "            \"calculation\": [\n",
    "                \"How do you calculate {concept}?\",\n",
    "                \"What is the formula for {concept}?\",\n",
    "                \"Walk me through calculating {concept}.\"\n",
    "            ],\n",
    "            \"strategy\": [\n",
    "                \"What strategies involve {concept}?\",\n",
    "                \"How can {concept} be used strategically?\",\n",
    "                \"What approaches work best with {concept}?\"\n",
    "            ],\n",
    "            \"risk\": [\n",
    "                \"What are the risks associated with {concept}?\",\n",
    "                \"What should I be careful about with {concept}?\",\n",
    "                \"What are potential downsides of {concept}?\"\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def generate_questions(self, topic: str, subtopic: str, count: int = 5) -> List[Dict]:\n",
    "        \"\"\"Generate questions for a subtopic using LLM.\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"You are a financial education expert. Generate {count} diverse educational questions about \"{subtopic}\" (under the topic \"{topic}\").\n",
    "\n",
    "Requirements:\n",
    "1. Questions should be clear and educational\n",
    "2. Include different types: definition, conceptual, comparison, example, application\n",
    "3. Range from beginner to advanced difficulty\n",
    "4. Focus on teaching financial concepts\n",
    "\n",
    "Output ONLY a numbered list of questions, nothing else.\n",
    "\n",
    "Example format:\n",
    "1. What is compound interest?\n",
    "2. How does compound interest differ from simple interest?\n",
    "3. Can you explain how compound interest affects long-term savings?\n",
    "\n",
    "Generate {count} questions about {subtopic}:\"\"\"\n",
    "        \n",
    "        response = self.model.generate(prompt)\n",
    "        questions = self._parse_questions(response, topic, subtopic)\n",
    "        \n",
    "        return questions\n",
    "    \n",
    "    def _parse_questions(self, response: str, topic: str, subtopic: str) -> List[Dict]:\n",
    "        \"\"\"Parse LLM response into structured questions.\"\"\"\n",
    "        questions = []\n",
    "        lines = response.strip().split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            # Remove numbering\n",
    "            if line and line[0].isdigit():\n",
    "                # Remove \"1.\", \"1)\", \"1:\" etc.\n",
    "                for sep in ['.', ')', ':']:\n",
    "                    if sep in line[:3]:\n",
    "                        line = line.split(sep, 1)[-1].strip()\n",
    "                        break\n",
    "            \n",
    "            if line and '?' in line and len(line) > 15:\n",
    "                question_type = self._classify_question_type(line)\n",
    "                questions.append({\n",
    "                    \"topic\": topic,\n",
    "                    \"subtopic\": subtopic,\n",
    "                    \"question\": line,\n",
    "                    \"question_type\": question_type\n",
    "                })\n",
    "        \n",
    "        return questions\n",
    "    \n",
    "    def _classify_question_type(self, question: str) -> str:\n",
    "        \"\"\"Classify the type of question.\"\"\"\n",
    "        question_lower = question.lower()\n",
    "        \n",
    "        if any(word in question_lower for word in [\"what is\", \"define\", \"what does\", \"what are\"]):\n",
    "            return \"definition\"\n",
    "        elif any(word in question_lower for word in [\"how does\", \"how do\", \"explain\", \"mechanism\"]):\n",
    "            return \"conceptual\"\n",
    "        elif any(word in question_lower for word in [\"difference\", \"compare\", \"versus\", \"vs\"]):\n",
    "            return \"comparison\"\n",
    "        elif any(word in question_lower for word in [\"example\", \"scenario\", \"illustrat\"]):\n",
    "            return \"example\"\n",
    "        elif any(word in question_lower for word in [\"apply\", \"practical\", \"use\"]):\n",
    "            return \"application\"\n",
    "        elif any(word in question_lower for word in [\"misconception\", \"myth\", \"wrong\"]):\n",
    "            return \"misconception\"\n",
    "        elif any(word in question_lower for word in [\"why\", \"important\", \"significance\"]):\n",
    "            return \"importance\"\n",
    "        elif any(word in question_lower for word in [\"calculate\", \"formula\", \"compute\"]):\n",
    "            return \"calculation\"\n",
    "        elif any(word in question_lower for word in [\"strategy\", \"approach\", \"tactic\"]):\n",
    "            return \"strategy\"\n",
    "        elif any(word in question_lower for word in [\"risk\", \"danger\", \"careful\", \"downside\"]):\n",
    "            return \"risk\"\n",
    "        else:\n",
    "            return random.choice(QUESTION_TYPES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad158974",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerWriterAgent(BaseAgent):\n",
    "    \"\"\"Generates structured, educational answers.\"\"\"\n",
    "    \n",
    "    def generate_answer(self, question_data: Dict) -> str:\n",
    "        \"\"\"Generate an educational answer for a question.\"\"\"\n",
    "        \n",
    "        topic = question_data[\"topic\"]\n",
    "        subtopic = question_data[\"subtopic\"]\n",
    "        question = question_data[\"question\"]\n",
    "        \n",
    "        prompt = f\"\"\"You are an expert financial educator. Answer the following question about {subtopic} ({topic}) in a clear, educational way.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Requirements:\n",
    "1. Provide a clear, accurate explanation\n",
    "2. Use simple language suitable for learners\n",
    "3. Include a practical example if applicable\n",
    "4. Keep the answer between 150-400 words\n",
    "5. Do NOT give personal financial advice\n",
    "6. Focus on education, not recommendations\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        answer = self.model.generate(prompt)\n",
    "        return answer.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cc24bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewerAgent(BaseAgent):\n",
    "    \"\"\"Reviews answers for quality and accuracy.\"\"\"\n",
    "    \n",
    "    def review(self, qa_pair: QAPair) -> Tuple[str, str]:\n",
    "        \"\"\"Review a Q&A pair and return (status, feedback).\"\"\"\n",
    "        \n",
    "        # Quick quality checks\n",
    "        if len(qa_pair.answer) < self.config.min_answer_length:\n",
    "            return \"rejected\", \"Answer too short\"\n",
    "        \n",
    "        if qa_pair.question.lower() in qa_pair.answer.lower()[:100]:\n",
    "            # Check if answer just repeats the question\n",
    "            pass\n",
    "        \n",
    "        # Check for problematic content\n",
    "        red_flags = [\n",
    "            \"i don't know\",\n",
    "            \"i cannot\",\n",
    "            \"i'm not sure\",\n",
    "            \"as an ai\",\n",
    "            \"i apologize\",\n",
    "            \"unfortunately\"\n",
    "        ]\n",
    "        \n",
    "        answer_lower = qa_pair.answer.lower()\n",
    "        for flag in red_flags:\n",
    "            if flag in answer_lower:\n",
    "                return \"needs_review\", f\"Contains uncertain language: {flag}\"\n",
    "        \n",
    "        # Check for financial advice warnings\n",
    "        advice_phrases = [\n",
    "            \"you should invest\",\n",
    "            \"i recommend buying\",\n",
    "            \"guaranteed returns\",\n",
    "            \"risk-free\",\n",
    "            \"get rich quick\"\n",
    "        ]\n",
    "        \n",
    "        for phrase in advice_phrases:\n",
    "            if phrase in answer_lower:\n",
    "                return \"rejected\", f\"Contains potential financial advice: {phrase}\"\n",
    "        \n",
    "        return \"verified\", \"Passed quality checks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b7e503",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AntiHallucinationAgent(BaseAgent):\n",
    "    \"\"\"Flags potentially incorrect or unsupported claims.\"\"\"\n",
    "    \n",
    "    def check(self, qa_pair: QAPair) -> Tuple[bool, str]:\n",
    "        \"\"\"Check for potential hallucinations.\"\"\"\n",
    "        \n",
    "        answer = qa_pair.answer.lower()\n",
    "        \n",
    "        # Check for specific numerical claims that might be hallucinated\n",
    "        suspicious_patterns = [\n",
    "            \"exactly \",\n",
    "            \"precisely \",\n",
    "            \"always returns\",\n",
    "            \"never fails\",\n",
    "            \"100%\",\n",
    "            \"guaranteed\"\n",
    "        ]\n",
    "        \n",
    "        for pattern in suspicious_patterns:\n",
    "            if pattern in answer:\n",
    "                return False, f\"Suspicious absolute claim: {pattern}\"\n",
    "        \n",
    "        # Check for made-up statistics\n",
    "        import re\n",
    "        percentages = re.findall(r'\\d+(?:\\.\\d+)?%', answer)\n",
    "        if len(percentages) > 3:\n",
    "            return False, \"Too many specific percentages (potential hallucination)\"\n",
    "        \n",
    "        return True, \"No hallucination flags detected\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d585148",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DifficultyClassifierAgent(BaseAgent):\n",
    "    \"\"\"Classifies difficulty of Q&A pairs.\"\"\"\n",
    "    \n",
    "    def classify(self, qa_pair: QAPair, topic_info: Dict) -> str:\n",
    "        \"\"\"Classify the difficulty of a Q&A pair.\"\"\"\n",
    "        \n",
    "        # Get allowed difficulty range for the topic\n",
    "        allowed_difficulties = topic_info.get(\"difficulty_range\", [\"beginner\", \"intermediate\", \"advanced\"])\n",
    "        \n",
    "        question_lower = qa_pair.question.lower()\n",
    "        answer_lower = qa_pair.answer.lower()\n",
    "        \n",
    "        # Advanced indicators\n",
    "        advanced_terms = [\n",
    "            \"derivative\", \"hedge\", \"arbitrage\", \"volatility\", \"correlation\",\n",
    "            \"beta\", \"alpha\", \"sharpe ratio\", \"monte carlo\", \"black-scholes\",\n",
    "            \"stochastic\", \"discounted cash flow\", \"wacc\", \"capm\"\n",
    "        ]\n",
    "        \n",
    "        # Intermediate indicators\n",
    "        intermediate_terms = [\n",
    "            \"portfolio\", \"diversification\", \"compound\", \"ratio\", \"margin\",\n",
    "            \"leverage\", \"equity\", \"bond yield\", \"pe ratio\", \"market cap\",\n",
    "            \"dividend yield\", \"asset allocation\"\n",
    "        ]\n",
    "        \n",
    "        # Count indicators\n",
    "        advanced_count = sum(1 for term in advanced_terms if term in answer_lower)\n",
    "        intermediate_count = sum(1 for term in intermediate_terms if term in answer_lower)\n",
    "        \n",
    "        if advanced_count >= 2 and \"advanced\" in allowed_difficulties:\n",
    "            return \"advanced\"\n",
    "        elif intermediate_count >= 2 or advanced_count >= 1:\n",
    "            if \"intermediate\" in allowed_difficulties:\n",
    "                return \"intermediate\"\n",
    "        \n",
    "        return \"beginner\" if \"beginner\" in allowed_difficulties else allowed_difficulties[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1d1fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeduplicatorAgent(BaseAgent):\n",
    "    \"\"\"Removes duplicate or highly similar Q&A pairs.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_manager: ModelManager, config: Config):\n",
    "        super().__init__(model_manager, config)\n",
    "        self.embedding_model = None\n",
    "        \n",
    "        if EMBEDDINGS_AVAILABLE:\n",
    "            try:\n",
    "                logger.info(\"üîÑ Loading embedding model for deduplication...\")\n",
    "                self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "                logger.info(\"‚úÖ Embedding model loaded\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not load embedding model: {e}\")\n",
    "    \n",
    "    def compute_hash(self, text: str) -> str:\n",
    "        \"\"\"Compute hash of text for quick duplicate detection.\"\"\"\n",
    "        # Normalize text\n",
    "        normalized = ' '.join(text.lower().split())\n",
    "        return hashlib.md5(normalized.encode()).hexdigest()\n",
    "    \n",
    "    def is_duplicate(self, new_qa: QAPair, existing_hashes: set) -> bool:\n",
    "        \"\"\"Check if Q&A pair is a duplicate.\"\"\"\n",
    "        q_hash = self.compute_hash(new_qa.question)\n",
    "        \n",
    "        if q_hash in existing_hashes:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def compute_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"Compute semantic similarity between two texts.\"\"\"\n",
    "        if self.embedding_model is None:\n",
    "            # Fallback to simple word overlap\n",
    "            words1 = set(text1.lower().split())\n",
    "            words2 = set(text2.lower().split())\n",
    "            intersection = words1.intersection(words2)\n",
    "            union = words1.union(words2)\n",
    "            return len(intersection) / len(union) if union else 0\n",
    "        \n",
    "        embeddings = self.embedding_model.encode([text1, text2])\n",
    "        similarity = util.cos_sim(embeddings[0], embeddings[1]).item()\n",
    "        return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0154af8",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "SECTION 7: MAIN GENERATOR CLASS\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f564d3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialDatasetGenerator:\n",
    "    \"\"\"Main orchestrator for dataset generation.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.state = GenerationState.load_checkpoint(config.checkpoint_file)\n",
    "        self.model_manager = None\n",
    "        \n",
    "        # Initialize agents (will be done after model loading)\n",
    "        self.topic_planner = None\n",
    "        self.question_writer = None\n",
    "        self.answer_writer = None\n",
    "        self.reviewer = None\n",
    "        self.anti_hallucination = None\n",
    "        self.difficulty_classifier = None\n",
    "        self.deduplicator = None\n",
    "        \n",
    "        # Statistics\n",
    "        self.stats = {\n",
    "            \"generated\": 0,\n",
    "            \"verified\": 0,\n",
    "            \"rejected\": 0,\n",
    "            \"duplicates\": 0,\n",
    "            \"hallucinations\": 0\n",
    "        }\n",
    "    \n",
    "    def initialize(self):\n",
    "        \"\"\"Initialize model and agents.\"\"\"\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(\"üöÄ FINANCIAL EDUCATION DATASET GENERATOR\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        # Load model\n",
    "        self.model_manager = ModelManager(self.config)\n",
    "        self.model_manager.load_model()\n",
    "        \n",
    "        # Initialize agents\n",
    "        self.topic_planner = TopicPlannerAgent(self.model_manager, self.config)\n",
    "        self.question_writer = QuestionWriterAgent(self.model_manager, self.config)\n",
    "        self.answer_writer = AnswerWriterAgent(self.model_manager, self.config)\n",
    "        self.reviewer = ReviewerAgent(self.model_manager, self.config)\n",
    "        self.anti_hallucination = AntiHallucinationAgent(self.model_manager, self.config)\n",
    "        self.difficulty_classifier = DifficultyClassifierAgent(self.model_manager, self.config)\n",
    "        self.deduplicator = DeduplicatorAgent(self.model_manager, self.config)\n",
    "        \n",
    "        logger.info(\"‚úÖ All agents initialized\")\n",
    "        \n",
    "        # Resume from checkpoint if exists\n",
    "        if self.state.total_generated > 0:\n",
    "            logger.info(f\"üìÇ Resuming from checkpoint: {self.state.total_generated} pairs already generated\")\n",
    "    \n",
    "    def generate_id(self, topic: str, subtopic: str, idx: int) -> str:\n",
    "        \"\"\"Generate unique ID for Q&A pair.\"\"\"\n",
    "        return f\"fin_edu_{topic[:3]}_{subtopic[:3]}_{idx}_{int(time.time())}\"\n",
    "    \n",
    "    def process_single_qa(self, question_data: Dict, topic_info: Dict) -> Optional[QAPair]:\n",
    "        \"\"\"Process a single Q&A through the pipeline.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Generate answer\n",
    "            answer = self.answer_writer.generate_answer(question_data)\n",
    "            \n",
    "            if not answer or len(answer) < self.config.min_answer_length:\n",
    "                self.stats[\"rejected\"] += 1\n",
    "                return None\n",
    "            \n",
    "            # Step 2: Create Q&A pair\n",
    "            qa_id = self.generate_id(\n",
    "                question_data[\"topic\"],\n",
    "                question_data[\"subtopic\"],\n",
    "                self.state.total_generated\n",
    "            )\n",
    "            \n",
    "            qa_pair = QAPair(\n",
    "                id=qa_id,\n",
    "                topic=question_data[\"topic\"],\n",
    "                subtopic=question_data[\"subtopic\"],\n",
    "                question=question_data[\"question\"],\n",
    "                answer=answer,\n",
    "                difficulty=\"pending\",\n",
    "                question_type=question_data[\"question_type\"]\n",
    "            )\n",
    "            \n",
    "            # Step 3: Check for duplicates\n",
    "            if self.deduplicator.is_duplicate(qa_pair, self.state.question_hashes):\n",
    "                self.stats[\"duplicates\"] += 1\n",
    "                return None\n",
    "            \n",
    "            # Step 4: Anti-hallucination check\n",
    "            is_valid, reason = self.anti_hallucination.check(qa_pair)\n",
    "            if not is_valid:\n",
    "                self.stats[\"hallucinations\"] += 1\n",
    "                return None\n",
    "            \n",
    "            # Step 5: Review\n",
    "            review_status, feedback = self.reviewer.review(qa_pair)\n",
    "            qa_pair.review_status = review_status\n",
    "            \n",
    "            if review_status == \"rejected\":\n",
    "                self.stats[\"rejected\"] += 1\n",
    "                return None\n",
    "            \n",
    "            # Step 6: Classify difficulty\n",
    "            qa_pair.difficulty = self.difficulty_classifier.classify(qa_pair, topic_info)\n",
    "            \n",
    "            # Update state\n",
    "            self.state.question_hashes.add(self.deduplicator.compute_hash(qa_pair.question))\n",
    "            self.state.answer_hashes.add(self.deduplicator.compute_hash(qa_pair.answer))\n",
    "            \n",
    "            self.stats[\"generated\"] += 1\n",
    "            if review_status == \"verified\":\n",
    "                self.stats[\"verified\"] += 1\n",
    "            \n",
    "            return qa_pair\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing Q&A: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def save_batch(self, qa_pairs: List[QAPair]):\n",
    "        \"\"\"Save a batch of Q&A pairs to file.\"\"\"\n",
    "        mode = 'a' if os.path.exists(self.config.output_file) else 'w'\n",
    "        \n",
    "        with open(self.config.output_file, mode, encoding='utf-8') as f:\n",
    "            for qa in qa_pairs:\n",
    "                f.write(qa.to_jsonl() + '\\n')\n",
    "        \n",
    "        # Save checkpoint\n",
    "        self.state.total_generated = self.stats[\"generated\"]\n",
    "        self.state.total_verified = self.stats[\"verified\"]\n",
    "        self.state.total_rejected = self.stats[\"rejected\"]\n",
    "        self.state.save_checkpoint(self.config.checkpoint_file)\n",
    "    \n",
    "    def print_progress(self):\n",
    "        \"\"\"Print current progress.\"\"\"\n",
    "        total = self.stats[\"generated\"]\n",
    "        target = self.config.target_dataset_size\n",
    "        percentage = (total / target) * 100\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üìä PROGRESS: {total:,} / {target:,} ({percentage:.1f}%)\")\n",
    "        print(f\"   ‚úÖ Verified: {self.stats['verified']:,}\")\n",
    "        print(f\"   ‚ùå Rejected: {self.stats['rejected']:,}\")\n",
    "        print(f\"   üîÑ Duplicates: {self.stats['duplicates']:,}\")\n",
    "        print(f\"   ‚ö†Ô∏è Hallucinations: {self.stats['hallucinations']:,}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    def generate_dataset(self):\n",
    "        \"\"\"Main generation loop.\"\"\"\n",
    "        \n",
    "        # Get curriculum\n",
    "        curriculum = self.topic_planner.process()\n",
    "        topics = list(curriculum.keys())\n",
    "        \n",
    "        # Calculate questions per subtopic to reach target\n",
    "        total_subtopics = sum(len(info[\"subtopics\"]) for info in curriculum.values())\n",
    "        questions_per_subtopic = max(10, self.config.target_dataset_size // total_subtopics)\n",
    "        \n",
    "        logger.info(f\"üìñ Curriculum: {len(topics)} topics, {total_subtopics} subtopics\")\n",
    "        logger.info(f\"üéØ Target: ~{questions_per_subtopic} Q&A per subtopic\")\n",
    "        \n",
    "        batch = []\n",
    "        generation_count = 0\n",
    "        \n",
    "        # Main generation loop\n",
    "        try:\n",
    "            for topic_idx, topic in enumerate(topics):\n",
    "                topic_info = curriculum[topic]\n",
    "                subtopics = topic_info[\"subtopics\"]\n",
    "                \n",
    "                for subtopic_idx, subtopic in enumerate(subtopics):\n",
    "                    logger.info(f\"\\nüìù Processing: {topic} > {subtopic}\")\n",
    "                    \n",
    "                    # Generate questions for this subtopic\n",
    "                    for q_batch in range(0, questions_per_subtopic, self.config.batch_size):\n",
    "                        # Generate a batch of questions\n",
    "                        questions = self.question_writer.generate_questions(\n",
    "                            topic, subtopic, \n",
    "                            count=min(self.config.batch_size, questions_per_subtopic - q_batch)\n",
    "                        )\n",
    "                        \n",
    "                        # Process each question\n",
    "                        for question_data in tqdm(questions, desc=f\"{subtopic[:20]}...\", leave=False):\n",
    "                            qa_pair = self.process_single_qa(question_data, topic_info)\n",
    "                            \n",
    "                            if qa_pair:\n",
    "                                batch.append(qa_pair)\n",
    "                                generation_count += 1\n",
    "                            \n",
    "                            # Save periodically\n",
    "                            if len(batch) >= self.config.save_interval:\n",
    "                                self.save_batch(batch)\n",
    "                                batch = []\n",
    "                                self.print_progress()\n",
    "                            \n",
    "                            # Clear GPU cache periodically\n",
    "                            if generation_count % self.config.clear_cache_interval == 0:\n",
    "                                self.model_manager.clear_cache()\n",
    "                            \n",
    "                            # Check if target reached\n",
    "                            if self.stats[\"generated\"] >= self.config.target_dataset_size:\n",
    "                                logger.info(\"üéâ Target dataset size reached!\")\n",
    "                                break\n",
    "                        \n",
    "                        if self.stats[\"generated\"] >= self.config.target_dataset_size:\n",
    "                            break\n",
    "                    \n",
    "                    if self.stats[\"generated\"] >= self.config.target_dataset_size:\n",
    "                        break\n",
    "                \n",
    "                if self.stats[\"generated\"] >= self.config.target_dataset_size:\n",
    "                    break\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            logger.info(\"\\n‚ö†Ô∏è Generation interrupted by user\")\n",
    "        \n",
    "        finally:\n",
    "            # Save remaining batch\n",
    "            if batch:\n",
    "                self.save_batch(batch)\n",
    "            \n",
    "            self.print_final_stats()\n",
    "    \n",
    "    def print_final_stats(self):\n",
    "        \"\"\"Print final statistics.\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"üèÅ GENERATION COMPLETE\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"üìä Total Q&A pairs generated: {self.stats['generated']:,}\")\n",
    "        print(f\"‚úÖ Verified: {self.stats['verified']:,}\")\n",
    "        print(f\"‚ùå Rejected: {self.stats['rejected']:,}\")\n",
    "        print(f\"üîÑ Duplicates removed: {self.stats['duplicates']:,}\")\n",
    "        print(f\"‚ö†Ô∏è Hallucinations caught: {self.stats['hallucinations']:,}\")\n",
    "        print(f\"\\nüìÅ Output file: {self.config.output_file}\")\n",
    "        \n",
    "        if os.path.exists(self.config.output_file):\n",
    "            file_size = os.path.getsize(self.config.output_file) / (1024 * 1024)\n",
    "            print(f\"üì¶ File size: {file_size:.2f} MB\")\n",
    "        \n",
    "        print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcb05c5",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "SECTION 8: UTILITY FUNCTIONS\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b7a1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_dataset(filepath: str) -> Dict[str, Any]:\n",
    "    \"\"\"Validate the generated dataset.\"\"\"\n",
    "    \n",
    "    print(f\"\\nüîç Validating dataset: {filepath}\")\n",
    "    \n",
    "    if not os.path.exists(filepath):\n",
    "        print(\"‚ùå File not found!\")\n",
    "        return {}\n",
    "    \n",
    "    stats = {\n",
    "        \"total_pairs\": 0,\n",
    "        \"by_topic\": defaultdict(int),\n",
    "        \"by_difficulty\": defaultdict(int),\n",
    "        \"by_status\": defaultdict(int),\n",
    "        \"avg_question_length\": 0,\n",
    "        \"avg_answer_length\": 0\n",
    "    }\n",
    "    \n",
    "    q_lengths = []\n",
    "    a_lengths = []\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                qa = json.loads(line.strip())\n",
    "                stats[\"total_pairs\"] += 1\n",
    "                stats[\"by_topic\"][qa.get(\"topic\", \"unknown\")] += 1\n",
    "                stats[\"by_difficulty\"][qa.get(\"difficulty\", \"unknown\")] += 1\n",
    "                stats[\"by_status\"][qa.get(\"review_status\", \"unknown\")] += 1\n",
    "                q_lengths.append(len(qa.get(\"question\", \"\")))\n",
    "                a_lengths.append(len(qa.get(\"answer\", \"\")))\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    \n",
    "    if q_lengths:\n",
    "        stats[\"avg_question_length\"] = sum(q_lengths) / len(q_lengths)\n",
    "    if a_lengths:\n",
    "        stats[\"avg_answer_length\"] = sum(a_lengths) / len(a_lengths)\n",
    "    \n",
    "    print(f\"\\nüìä Dataset Statistics:\")\n",
    "    print(f\"   Total pairs: {stats['total_pairs']:,}\")\n",
    "    print(f\"   Avg question length: {stats['avg_question_length']:.0f} chars\")\n",
    "    print(f\"   Avg answer length: {stats['avg_answer_length']:.0f} chars\")\n",
    "    print(f\"\\nüìö By Topic:\")\n",
    "    for topic, count in sorted(stats[\"by_topic\"].items(), key=lambda x: -x[1]):\n",
    "        print(f\"   {topic}: {count:,}\")\n",
    "    print(f\"\\nüìà By Difficulty:\")\n",
    "    for diff, count in stats[\"by_difficulty\"].items():\n",
    "        print(f\"   {diff}: {count:,}\")\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cda9448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dataset(filepath: str, n: int = 5):\n",
    "    \"\"\"Print sample entries from the dataset.\"\"\"\n",
    "    \n",
    "    print(f\"\\nüìã Sample entries from {filepath}:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    samples = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= n:\n",
    "                break\n",
    "            samples.append(json.loads(line.strip()))\n",
    "    \n",
    "    for i, qa in enumerate(samples, 1):\n",
    "        print(f\"\\n--- Sample {i} ---\")\n",
    "        print(f\"Topic: {qa.get('topic')} > {qa.get('subtopic')}\")\n",
    "        print(f\"Difficulty: {qa.get('difficulty')}\")\n",
    "        print(f\"Question: {qa.get('question')}\")\n",
    "        print(f\"Answer: {qa.get('answer')[:300]}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dcb11e",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "SECTION 9: MAIN EXECUTION\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff659b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main entry point.\"\"\"\n",
    "    \n",
    "    print(\"\"\"\n",
    "    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "    ‚ïë     FINANCIAL EDUCATION DATASET GENERATOR                     ‚ïë\n",
    "    ‚ïë     Multi-Agent System for Q&A Generation                     ‚ïë\n",
    "    ‚ïë     Optimized for Google Colab                                ‚ïë\n",
    "    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "    \"\"\")\n",
    "    \n",
    "    # Check GPU availability\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"üéÆ GPU Detected: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No GPU detected. Running on CPU (will be slower)\")\n",
    "    \n",
    "    # Initialize configuration\n",
    "    config = Config()\n",
    "    \n",
    "    # Allow user to modify target size\n",
    "    print(f\"\\nüìä Current settings:\")\n",
    "    print(f\"   Target dataset size: {config.target_dataset_size:,}\")\n",
    "    print(f\"   Model: {config.model_name}\")\n",
    "    print(f\"   Quantization: {'4-bit (NF4)' if config.use_quantization else 'None'}\")\n",
    "    print(f\"   Output file: {config.output_file}\")\n",
    "    \n",
    "    # Create generator and run\n",
    "    generator = FinancialDatasetGenerator(config)\n",
    "    generator.initialize()\n",
    "    generator.generate_dataset()\n",
    "    \n",
    "    # Validate output\n",
    "    if os.path.exists(config.output_file):\n",
    "        validate_dataset(config.output_file)\n",
    "        sample_dataset(config.output_file, n=3)\n",
    "    \n",
    "    print(\"\\n‚úÖ Generation complete! Your dataset is ready.\")\n",
    "    print(f\"üìÅ File location: {config.output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90cc967",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
