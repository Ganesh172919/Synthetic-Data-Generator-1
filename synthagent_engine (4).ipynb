{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f47d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "===============================================================================\n",
    "SynthAgent Engine - Multi-Agent Synthetic Data Generator\n",
    "\n",
    "Purpose: Generate high-quality financial Q&A pairs using open-source LLMs\n",
    "\n",
    "Constraints:\n",
    "- 100% Free (Google Colab Free Tier)\n",
    "- Open-Source LLMs via HuggingFace\n",
    "- <12GB RAM usage with 4-bit quantization\n",
    "- LangChain + LangGraph multi-agent architecture\n",
    "\n",
    "Author: Generated from collaborative development\n",
    "===============================================================================\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3522ed",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "CELL 1: ENVIRONMENT SETUP\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd11e00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Installing dependencies...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2099b52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core ML / LLM packages (CUDA 11.8)\n",
    "!pip install -q torch torchvision torchaudio \\\n",
    "  --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869daf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q \"transformers>=4.36.0\" \"accelerate>=0.25.0\" \"bitsandbytes>=0.41.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e142c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain ecosystem\n",
    "!pip install -q \"langchain>=0.3.0\" \"langchain-community>=0.3.0\" \"langchain-huggingface>=0.1.0\"\n",
    "!pip install -q \"langgraph>=0.2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae5e0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structured output & validation\n",
    "!pip install -q \"pydantic>=2.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487ab196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data handling\n",
    "!pip install -q pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8ee52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress & display\n",
    "!pip install -q tqdm rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7afdf9e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"All dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839b8d26",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "CELL 2: IMPORTS AND CONFIGURATION\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef66d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Any, Literal, TypedDict, Annotated\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a66d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from rich.panel import Panel\n",
    "from rich.progress import Progress, SpinnerColumn, TextColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acd94aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain imports\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_huggingface import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7949338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1c7ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a58974",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "console = Console()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca3888c",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "CONFIGURATION\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18545847",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"Global configuration for SynthAgent Engine\"\"\"\n",
    "\n",
    "    # Model settings\n",
    "    MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "    USE_4BIT = True\n",
    "    MAX_NEW_TOKENS = 512\n",
    "    TEMPERATURE = 0.7\n",
    "\n",
    "    # Generation settings\n",
    "    TARGET_SAMPLES = 10000\n",
    "    BATCH_SIZE = 20\n",
    "    SAMPLES_PER_LLM_CALL = 5\n",
    "\n",
    "    # Quality thresholds\n",
    "    MIN_QUALITY_SCORE = 7.0\n",
    "    MAX_CORRECTION_ROUNDS = 2\n",
    "    MAX_CONSECUTIVE_FAILURES = 5\n",
    "\n",
    "    # HITL settings\n",
    "    HITL_CHECKPOINT_INTERVAL = 500\n",
    "    ENABLE_HITL = True\n",
    "\n",
    "    # Output settings\n",
    "    OUTPUT_DIR = \"/content/output\"\n",
    "    CSV_FILENAME = \"financial_qa_dataset.csv\"\n",
    "\n",
    "    # Seed for reproducibility\n",
    "    RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ef49ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "random.seed(config.RANDOM_SEED)\n",
    "np.random.seed(config.RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73b55e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Configuration loaded:\")\n",
    "print(f\"   Model: {config.MODEL_ID}\")\n",
    "print(f\"   Target samples: {config.TARGET_SAMPLES}\")\n",
    "print(f\"   Batch size: {config.BATCH_SIZE}\")\n",
    "print(f\"   Quality threshold: {config.MIN_QUALITY_SCORE}/10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97128b0e",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "CELL 3: PYDANTIC MODELS\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7a218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskType(str, Enum):\n",
    "    \"\"\"Supported task types for generation\"\"\"\n",
    "    QA_SINGLE = \"question_answering_single\"\n",
    "    QA_MULTI = \"question_answering_multi\"\n",
    "    REASONING = \"chain_of_thought\"\n",
    "    INSTRUCTION = \"instruction_following\"\n",
    "    CLASSIFICATION = \"classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477fc7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DifficultyLevel(str, Enum):\n",
    "    \"\"\"Difficulty levels for generated content\"\"\"\n",
    "    BEGINNER = \"beginner\"\n",
    "    INTERMEDIATE = \"intermediate\"\n",
    "    ADVANCED = \"advanced\"\n",
    "    EXPERT = \"expert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1fbef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialCategory(str, Enum):\n",
    "    \"\"\"Financial domain categories\"\"\"\n",
    "    INVESTING = \"investing\"\n",
    "    BANKING = \"banking\"\n",
    "    TAXATION = \"taxation\"\n",
    "    ACCOUNTING = \"accounting\"\n",
    "    INSURANCE = \"insurance\"\n",
    "    RETIREMENT = \"retirement\"\n",
    "    REAL_ESTATE = \"real_estate\"\n",
    "    CRYPTO = \"cryptocurrency\"\n",
    "    CORPORATE_FINANCE = \"corporate_finance\"\n",
    "    PERSONAL_FINANCE = \"personal_finance\"\n",
    "    MARKETS = \"stock_markets\"\n",
    "    DERIVATIVES = \"derivatives\"\n",
    "    RISK_MANAGEMENT = \"risk_management\"\n",
    "    REGULATIONS = \"financial_regulations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecff70fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerationRequirements(BaseModel):\n",
    "    \"\"\"Parsed requirements from user input\"\"\"\n",
    "    domain: str = Field(default=\"finance\", description=\"Target domain\")\n",
    "    task_type: TaskType = Field(default=TaskType.QA_SINGLE)\n",
    "    target_size: int = Field(default=10000, ge=100, le=100000)\n",
    "    difficulty_distribution: Dict[str, float] = Field(\n",
    "        default={\"beginner\": 0.2, \"intermediate\": 0.4, \"advanced\": 0.3, \"expert\": 0.1}\n",
    "    )\n",
    "    categories: List[str] = Field(default_factory=list)\n",
    "    language: str = Field(default=\"english\")\n",
    "    special_requirements: List[str] = Field(default_factory=list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59cec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAPair(BaseModel):\n",
    "    \"\"\"Single question-answer pair with metadata\"\"\"\n",
    "    question: str = Field(..., min_length=10, description=\"The question\")\n",
    "    answer: str = Field(..., min_length=20, description=\"Detailed answer\")\n",
    "    category: str = Field(..., description=\"Financial category\")\n",
    "    difficulty: str = Field(..., description=\"Difficulty level\")\n",
    "    reasoning: Optional[str] = Field(None, description=\"Chain of thought reasoning\")\n",
    "    keywords: List[str] = Field(default_factory=list, description=\"Key concepts\")\n",
    "\n",
    "    @validator('question')\n",
    "    def question_must_end_with_punctuation(cls, v):\n",
    "        if not v.strip().endswith('?'):\n",
    "            v = v.strip() + '?'\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04c4c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QualityScore(BaseModel):\n",
    "    \"\"\"Quality assessment for a QA pair or batch\"\"\"\n",
    "    coherence: float = Field(..., ge=0, le=10, description=\"Logical coherence\")\n",
    "    accuracy: float = Field(..., ge=0, le=10, description=\"Factual accuracy\")\n",
    "    completeness: float = Field(..., ge=0, le=10, description=\"Answer completeness\")\n",
    "    clarity: float = Field(..., ge=0, le=10, description=\"Language clarity\")\n",
    "    relevance: float = Field(..., ge=0, le=10, description=\"Domain relevance\")\n",
    "    overall: float = Field(..., ge=0, le=10, description=\"Overall quality\")\n",
    "    feedback: str = Field(default=\"\", description=\"Improvement suggestions\")\n",
    "    passed: bool = Field(default=True, description=\"Meets quality threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdee7739",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerationBatch(BaseModel):\n",
    "    \"\"\"Batch of generated QA pairs\"\"\"\n",
    "    batch_id: int\n",
    "    samples: List[QAPair]\n",
    "    quality_score: Optional[QualityScore] = None\n",
    "    correction_rounds: int = Field(default=0)\n",
    "    status: str = Field(default=\"pending\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8143dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangGraph State Definition\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"State passed between agents in the graph\"\"\"\n",
    "    user_input: str\n",
    "    requirements: Dict\n",
    "    schema: Dict\n",
    "    context: Dict\n",
    "    current_batch: Dict\n",
    "    generated_samples: List[Dict]\n",
    "    quality_scores: List[Dict]\n",
    "    total_generated: int\n",
    "    total_accepted: int\n",
    "    errors: List[str]\n",
    "    current_step: str\n",
    "    hitl_pause: bool\n",
    "    final_output: str\n",
    "    consecutive_failed_batches: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd032c8f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"Pydantic models defined\")\n",
    "print(f\"   - GenerationRequirements: Parses user input\")\n",
    "print(f\"   - QAPair: Individual Q&A sample\")\n",
    "print(f\"   - QualityScore: Quality metrics\")\n",
    "print(f\"   - GenerationBatch: Batch container\")\n",
    "print(f\"   - AgentState: LangGraph workflow state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceac8c30",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "CELL 4: MODEL LOADING\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8072d0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(Panel.fit(\"Loading Mistral-7B-Instruct with 4-bit quantization...\",\n",
    "                        title=\"Model Loading\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39152d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Available memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected - will use CPU (much slower)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c343bce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c05217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "print(\"\\nLoading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config.MODEL_ID,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af47aebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with quantization\n",
    "print(\"Loading model (this takes 3-5 minutes)...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9edc13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text generation pipeline\n",
    "print(\"Creating generation pipeline...\")\n",
    "text_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=config.MAX_NEW_TOKENS,\n",
    "    temperature=config.TEMPERATURE,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c061c681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap for LangChain\n",
    "llm = HuggingFacePipeline(pipeline=text_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17300d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory cleanup\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed050a85",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "console.print(Panel.fit(\"Model loaded successfully!\", title=\"Complete\", style=\"green\"))\n",
    "print(f\"   Memory used: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669fb450",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "CELL 5: FINANCIAL DOMAIN KNOWLEDGE\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec25f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINANCIAL_KNOWLEDGE = {\n",
    "    \"categories\": {\n",
    "        \"investing\": {\n",
    "            \"topics\": [\"stocks\", \"bonds\", \"ETFs\", \"mutual funds\", \"portfolio diversification\",\n",
    "                      \"asset allocation\", \"value investing\", \"growth investing\", \"dividend investing\",\n",
    "                      \"index funds\", \"market timing\", \"dollar-cost averaging\"],\n",
    "            \"concepts\": [\"P/E ratio\", \"market capitalization\", \"dividend yield\", \"beta\",\n",
    "                        \"alpha\", \"Sharpe ratio\", \"bull market\", \"bear market\", \"volatility\"],\n",
    "        },\n",
    "        \"banking\": {\n",
    "            \"topics\": [\"savings accounts\", \"checking accounts\", \"CDs\", \"money market accounts\",\n",
    "                      \"interest rates\", \"FDIC insurance\", \"overdraft protection\", \"wire transfers\"],\n",
    "            \"concepts\": [\"APY\", \"APR\", \"compound interest\", \"liquidity\", \"minimum balance\"],\n",
    "        },\n",
    "        \"taxation\": {\n",
    "            \"topics\": [\"income tax\", \"capital gains\", \"tax deductions\", \"tax credits\",\n",
    "                      \"tax brackets\", \"filing status\", \"W-2\", \"1099\", \"estimated taxes\"],\n",
    "            \"concepts\": [\"marginal tax rate\", \"effective tax rate\", \"tax-deferred\", \"tax-exempt\"],\n",
    "        },\n",
    "        \"retirement\": {\n",
    "            \"topics\": [\"401(k)\", \"IRA\", \"Roth IRA\", \"pension\", \"Social Security\",\n",
    "                      \"retirement planning\", \"required minimum distributions\", \"catch-up contributions\"],\n",
    "            \"concepts\": [\"compound growth\", \"employer match\", \"vesting\", \"early withdrawal penalty\"],\n",
    "        },\n",
    "        \"personal_finance\": {\n",
    "            \"topics\": [\"budgeting\", \"emergency fund\", \"debt management\", \"credit score\",\n",
    "                      \"net worth\", \"financial goals\", \"saving strategies\", \"spending tracking\"],\n",
    "            \"concepts\": [\"50/30/20 rule\", \"debt-to-income ratio\", \"sinking fund\", \"pay yourself first\"],\n",
    "        },\n",
    "        \"stock_markets\": {\n",
    "            \"topics\": [\"NYSE\", \"NASDAQ\", \"market orders\", \"limit orders\", \"stop-loss\",\n",
    "                      \"trading hours\", \"after-hours trading\", \"IPO\", \"stock splits\"],\n",
    "            \"concepts\": [\"bid-ask spread\", \"market makers\", \"trading volume\", \"circuit breakers\"],\n",
    "        },\n",
    "        \"corporate_finance\": {\n",
    "            \"topics\": [\"financial statements\", \"balance sheet\", \"income statement\", \"cash flow\",\n",
    "                      \"EBITDA\", \"working capital\", \"capital structure\", \"mergers and acquisitions\"],\n",
    "            \"concepts\": [\"ROE\", \"ROA\", \"debt-to-equity\", \"current ratio\", \"quick ratio\"],\n",
    "        },\n",
    "        \"risk_management\": {\n",
    "            \"topics\": [\"diversification\", \"hedging\", \"insurance\", \"asset allocation\",\n",
    "                      \"risk tolerance\", \"systematic risk\", \"unsystematic risk\"],\n",
    "            \"concepts\": [\"standard deviation\", \"VaR\", \"correlation\", \"beta coefficient\"],\n",
    "        },\n",
    "    },\n",
    "\n",
    "    \"difficulty_guidelines\": {\n",
    "        \"beginner\": \"Basic concepts, simple explanations, everyday financial decisions\",\n",
    "        \"intermediate\": \"Moderate complexity, some calculations, investment strategies\",\n",
    "        \"advanced\": \"Complex scenarios, detailed analysis, professional-level knowledge\",\n",
    "        \"expert\": \"Highly technical, regulatory knowledge, institutional finance\",\n",
    "    },\n",
    "\n",
    "    \"question_templates\": [\n",
    "        \"What is {concept} and why is it important for {context}?\",\n",
    "        \"How does {concept} affect {related_topic}?\",\n",
    "        \"What are the advantages and disadvantages of {topic}?\",\n",
    "        \"How should someone approach {scenario}?\",\n",
    "        \"What factors should be considered when {action}?\",\n",
    "        \"Explain the difference between {concept1} and {concept2}.\",\n",
    "        \"What are the tax implications of {financial_action}?\",\n",
    "        \"How can {strategy} help achieve {financial_goal}?\",\n",
    "        \"What risks are associated with {investment_type}?\",\n",
    "        \"When is the best time to {financial_decision}?\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba28a942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot examples for high-quality generation\n",
    "FEW_SHOT_EXAMPLES = [\n",
    "    {\n",
    "        \"question\": \"What is the difference between a traditional IRA and a Roth IRA?\",\n",
    "        \"answer\": \"A traditional IRA and Roth IRA differ primarily in their tax treatment. With a traditional IRA, contributions may be tax-deductible, and you pay taxes when you withdraw funds in retirement. With a Roth IRA, contributions are made with after-tax dollars, but qualified withdrawals in retirement are completely tax-free. The choice between them depends on whether you expect to be in a higher or lower tax bracket in retirement. Traditional IRAs also have required minimum distributions (RMDs) starting at age 73, while Roth IRAs have no RMDs during the owner's lifetime.\",\n",
    "        \"category\": \"retirement\",\n",
    "        \"difficulty\": \"intermediate\",\n",
    "        \"keywords\": [\"IRA\", \"Roth IRA\", \"traditional IRA\", \"tax-deferred\", \"tax-free\", \"RMD\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does compound interest work and why is it called the 'eighth wonder of the world'?\",\n",
    "        \"answer\": \"Compound interest is the process where interest is calculated not only on the initial principal but also on the accumulated interest from previous periods. For example, if you invest $1,000 at 10% annual interest, after year one you have $1,100. In year two, you earn 10% on $1,100 (not just $1,000), giving you $1,210. This compounding effect accelerates wealth growth exponentially over time. Albert Einstein reportedly called it the 'eighth wonder of the world' because those who understand it earn it, while those who don't pay it. Starting early maximizes the benefit, as time is the most powerful factor in compound growth.\",\n",
    "        \"category\": \"investing\",\n",
    "        \"difficulty\": \"beginner\",\n",
    "        \"keywords\": [\"compound interest\", \"principal\", \"exponential growth\", \"time value of money\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the debt-to-equity ratio and how do investors use it to evaluate companies?\",\n",
    "        \"answer\": \"The debt-to-equity (D/E) ratio measures a company's financial leverage by dividing total liabilities by shareholders' equity. A D/E ratio of 1.0 means the company has equal amounts of debt and equity financing. Higher ratios indicate more debt financing, which can amplify returns but also increases financial risk. Investors use this metric to: 1) Compare companies within the same industry (capital-intensive industries like utilities typically have higher D/E ratios than tech companies), 2) Assess bankruptcy risk during economic downturns, 3) Evaluate management's capital allocation strategy. Generally, a D/E ratio below 2.0 is considered conservative for most industries.\",\n",
    "        \"category\": \"corporate_finance\",\n",
    "        \"difficulty\": \"advanced\",\n",
    "        \"keywords\": [\"debt-to-equity ratio\", \"leverage\", \"financial risk\", \"capital structure\"]\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2828388d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"Financial knowledge base loaded\")\n",
    "print(f\"   Categories: {len(FINANCIAL_KNOWLEDGE['categories'])}\")\n",
    "print(f\"   Question templates: {len(FINANCIAL_KNOWLEDGE['question_templates'])}\")\n",
    "print(f\"   Few-shot examples: {len(FEW_SHOT_EXAMPLES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1fa68d",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "CELL 6: AGENT PROMPT TEMPLATES\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d749b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirement Parser Agent Prompt\n",
    "REQUIREMENT_PARSER_PROMPT = \"\"\"<s>[INST] You are a requirements analysis expert. Parse the user's request and extract structured generation requirements.\n",
    "\n",
    "USER REQUEST: {user_input}\n",
    "\n",
    "Extract and return a JSON object with these fields:\n",
    "- domain: The target domain (default: \"finance\")\n",
    "- task_type: One of [question_answering_single, reasoning, instruction_following]\n",
    "- target_size: Number of samples to generate (default: 10000)\n",
    "- categories: List of specific categories to focus on\n",
    "- difficulty_distribution: Dict with beginner/intermediate/advanced/expert percentages\n",
    "- special_requirements: Any special instructions\n",
    "\n",
    "Return ONLY valid JSON, no other text.\n",
    "\n",
    "Example output:\n",
    "{{\"domain\": \"finance\", \"task_type\": \"question_answering_single\", \"target_size\": 10000, \"categories\": [\"investing\", \"retirement\", \"taxation\"], \"difficulty_distribution\": {{\"beginner\": 0.25, \"intermediate\": 0.40, \"advanced\": 0.25, \"expert\": 0.10}}, \"special_requirements\": [\"include calculations\", \"practical examples\"]}}\n",
    "\n",
    "JSON output: [/INST]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d27f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema Designer Agent Prompt\n",
    "SCHEMA_DESIGNER_PROMPT = \"\"\"<s>[INST] You are a data schema expert. Design the output schema for financial Q&A pairs.\n",
    "\n",
    "REQUIREMENTS: {requirements}\n",
    "\n",
    "Create a schema that includes:\n",
    "1. Question format guidelines\n",
    "2. Answer structure requirements\n",
    "3. Required metadata fields\n",
    "4. Quality criteria\n",
    "\n",
    "Return a JSON schema specification.\n",
    "\n",
    "Output: [/INST]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63b4e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context Builder Agent Prompt\n",
    "CONTEXT_BUILDER_PROMPT = \"\"\"<s>[INST] You are a financial domain expert. Build rich context for generating Q&A pairs.\n",
    "\n",
    "CATEGORY: {category}\n",
    "DIFFICULTY: {difficulty}\n",
    "AVAILABLE TOPICS: {topics}\n",
    "AVAILABLE CONCEPTS: {concepts}\n",
    "\n",
    "Create context that includes:\n",
    "1. Relevant background information\n",
    "2. Key concepts to incorporate\n",
    "3. Real-world scenarios\n",
    "4. Common misconceptions to address\n",
    "\n",
    "Return the context as a JSON object with fields: background, key_concepts, scenarios, misconceptions.\n",
    "\n",
    "Context JSON: [/INST]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a264f855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Master Data Generator Agent Prompt\n",
    "GENERATOR_PROMPT = \"\"\"<s>[INST] You are an expert financial educator creating high-quality Q&A pairs for training AI models.\n",
    "\n",
    "TASK: Generate {num_samples} diverse, high-quality financial question-answer pairs.\n",
    "\n",
    "CATEGORY: {category}\n",
    "DIFFICULTY LEVEL: {difficulty}\n",
    "CONTEXT: {context}\n",
    "\n",
    "REQUIREMENTS:\n",
    "1. Questions should be clear, specific, and educational\n",
    "2. Answers must be accurate, comprehensive (2-4 sentences minimum), and practical\n",
    "3. Include relevant financial terminology\n",
    "4. Vary question types (what, how, why, when, compare)\n",
    "5. Answers should explain concepts, not just define them\n",
    "\n",
    "FEW-SHOT EXAMPLES:\n",
    "{examples}\n",
    "\n",
    "Generate {num_samples} Q&A pairs in this exact JSON format:\n",
    "[\n",
    "  {{\"question\": \"...\", \"answer\": \"...\", \"category\": \"{category}\", \"difficulty\": \"{difficulty}\", \"keywords\": [\"...\", \"...\"]}},\n",
    "  ...\n",
    "]\n",
    "\n",
    "Output ONLY the JSON array, no other text:\n",
    "[/INST]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a008b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality Controller Agent Prompt\n",
    "QUALITY_CONTROLLER_PROMPT = \"\"\"<s>[INST] You are a quality assurance expert reviewing financial Q&A pairs.\n",
    "\n",
    "REVIEW THESE Q&A PAIRS:\n",
    "{samples}\n",
    "\n",
    "SCORING CRITERIA (0-10 for each):\n",
    "1. Coherence: Is the answer logically structured?\n",
    "2. Accuracy: Is the financial information correct?\n",
    "3. Completeness: Does the answer fully address the question?\n",
    "4. Clarity: Is the language clear and professional?\n",
    "5. Relevance: Is this appropriate for the stated category/difficulty?\n",
    "\n",
    "Return a JSON object with:\n",
    "{{\"coherence\": X, \"accuracy\": X, \"completeness\": X, \"clarity\": X, \"relevance\": X, \"overall\": X, \"feedback\": \"specific improvement suggestions\", \"passed\": true/false}}\n",
    "\n",
    "A sample passes if overall >= 7.0\n",
    "\n",
    "Quality assessment JSON: [/INST]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f3ea26",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"Agent prompts configured\")\n",
    "print(\"   - RequirementParserAgent: Extracts structured requirements\")\n",
    "print(\"   - SchemaDesignerAgent: Designs output schema\")\n",
    "print(\"   - ContextBuilderAgent: Creates domain context\")\n",
    "print(\"   - MasterGeneratorAgent: Generates Q&A pairs\")\n",
    "print(\"   - QualityControllerAgent: Validates quality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e52421",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "CELL 7: AGENT IMPLEMENTATIONS\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244b226d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64d4442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json(text: str) -> Any:\n",
    "    \"\"\"Safely extract JSON from LLM response\"\"\"\n",
    "    try:\n",
    "        # Try direct parse first\n",
    "        return json.loads(text)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Try to find JSON in the text\n",
    "    patterns = [\n",
    "        r'\\[[\\s\\S]*\\]',  # JSON array\n",
    "        r'\\{[\\s\\S]*\\}',  # JSON object\n",
    "    ]\n",
    "\n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, text)\n",
    "        for match in matches:\n",
    "            try:\n",
    "                return json.loads(match)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de44781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_llm_output(text: str) -> str:\n",
    "    \"\"\"Clean LLM output artifacts\"\"\"\n",
    "    # Remove common artifacts\n",
    "    text = text.replace('</s>', '').replace('<s>', '')\n",
    "    text = text.replace('[INST]', '').replace('[/INST]', '')\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c74b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RequirementParserAgent:\n",
    "    \"\"\"Parses natural language input into structured requirements\"\"\"\n",
    "\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.prompt = PromptTemplate.from_template(REQUIREMENT_PARSER_PROMPT)\n",
    "\n",
    "    def invoke(self, user_input: str) -> Dict:\n",
    "        \"\"\"Parse user input into structured requirements\"\"\"\n",
    "        try:\n",
    "            formatted_prompt = self.prompt.format(user_input=user_input)\n",
    "            response = self.llm.invoke(formatted_prompt)\n",
    "            response = clean_llm_output(response)\n",
    "\n",
    "            parsed = extract_json(response)\n",
    "            if parsed:\n",
    "                return parsed\n",
    "\n",
    "            # Default fallback\n",
    "            return {\n",
    "                \"domain\": \"finance\",\n",
    "                \"task_type\": \"question_answering_single\",\n",
    "                \"target_size\": config.TARGET_SAMPLES,\n",
    "                \"categories\": list(FINANCIAL_KNOWLEDGE[\"categories\"].keys()),\n",
    "                \"difficulty_distribution\": {\"beginner\": 0.25, \"intermediate\": 0.40,\n",
    "                                           \"advanced\": 0.25, \"expert\": 0.10},\n",
    "                \"special_requirements\": []\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Parser warning: {e}, using defaults\")\n",
    "            return {\n",
    "                \"domain\": \"finance\",\n",
    "                \"task_type\": \"question_answering_single\",\n",
    "                \"target_size\": config.TARGET_SAMPLES,\n",
    "                \"categories\": list(FINANCIAL_KNOWLEDGE[\"categories\"].keys()),\n",
    "                \"difficulty_distribution\": {\"beginner\": 0.25, \"intermediate\": 0.40,\n",
    "                                           \"advanced\": 0.25, \"expert\": 0.10},\n",
    "                \"special_requirements\": []\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a78ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextBuilderAgent:\n",
    "    \"\"\"Builds rich domain context for generation\"\"\"\n",
    "\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.prompt = PromptTemplate.from_template(CONTEXT_BUILDER_PROMPT)\n",
    "\n",
    "    def invoke(self, category: str, difficulty: str) -> Dict:\n",
    "        \"\"\"Build context for the given category and difficulty\"\"\"\n",
    "        cat_data = FINANCIAL_KNOWLEDGE[\"categories\"].get(\n",
    "            category,\n",
    "            FINANCIAL_KNOWLEDGE[\"categories\"][\"investing\"]\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            formatted_prompt = self.prompt.format(\n",
    "                category=category,\n",
    "                difficulty=difficulty,\n",
    "                topics=\", \".join(cat_data.get(\"topics\", [])),\n",
    "                concepts=\", \".join(cat_data.get(\"concepts\", []))\n",
    "            )\n",
    "\n",
    "            response = self.llm.invoke(formatted_prompt)\n",
    "            response = clean_llm_output(response)\n",
    "\n",
    "            parsed = extract_json(response)\n",
    "            if parsed:\n",
    "                return parsed\n",
    "        except Exception as e:\n",
    "            print(f\"Context builder warning: {e}\")\n",
    "\n",
    "        # Fallback context\n",
    "        return {\n",
    "            \"background\": f\"Financial knowledge about {category}\",\n",
    "            \"key_concepts\": cat_data.get(\"concepts\", []),\n",
    "            \"scenarios\": cat_data.get(\"topics\", []),\n",
    "            \"misconceptions\": []\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a945f37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MasterDataGeneratorAgent:\n",
    "    \"\"\"Generates high-quality Q&A pairs in batches\"\"\"\n",
    "\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.prompt = PromptTemplate.from_template(GENERATOR_PROMPT)\n",
    "\n",
    "    def invoke(self, category: str, difficulty: str, context: Dict,\n",
    "               num_samples: int = 5) -> List[Dict]:\n",
    "        \"\"\"Generate a batch of Q&A pairs\"\"\"\n",
    "\n",
    "        # Format few-shot examples\n",
    "        examples_str = \"\"\n",
    "        for ex in FEW_SHOT_EXAMPLES[:2]:\n",
    "            examples_str += f\"\\nQ: {ex['question']}\\nA: {ex['answer']}\\nCategory: {ex['category']}, Difficulty: {ex['difficulty']}\\n\"\n",
    "\n",
    "        try:\n",
    "            formatted_prompt = self.prompt.format(\n",
    "                num_samples=num_samples,\n",
    "                category=category,\n",
    "                difficulty=difficulty,\n",
    "                context=json.dumps(context),\n",
    "                examples=examples_str\n",
    "            )\n",
    "\n",
    "            response = self.llm.invoke(formatted_prompt)\n",
    "            response = clean_llm_output(response)\n",
    "\n",
    "            parsed = extract_json(response)\n",
    "            if parsed and isinstance(parsed, list):\n",
    "                # Validate and clean each sample\n",
    "                valid_samples = []\n",
    "                for item in parsed:\n",
    "                    if isinstance(item, dict) and 'question' in item and 'answer' in item:\n",
    "                        sample = {\n",
    "                            'question': str(item.get('question', '')).strip(),\n",
    "                            'answer': str(item.get('answer', '')).strip(),\n",
    "                            'category': category,\n",
    "                            'difficulty': difficulty,\n",
    "                            'keywords': item.get('keywords', []),\n",
    "                            'reasoning': item.get('reasoning', '')\n",
    "                        }\n",
    "                        if len(sample['question']) > 10 and len(sample['answer']) > 20:\n",
    "                            valid_samples.append(sample)\n",
    "\n",
    "                if valid_samples:\n",
    "                    return valid_samples\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Generator warning: {e}\")\n",
    "\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d314143a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QualityControllerAgent:\n",
    "    \"\"\"Validates and scores generated Q&A pairs\"\"\"\n",
    "\n",
    "    def __init__(self, llm, threshold: float = 7.0):\n",
    "        self.llm = llm\n",
    "        self.threshold = threshold\n",
    "        self.prompt = PromptTemplate.from_template(QUALITY_CONTROLLER_PROMPT)\n",
    "\n",
    "    def invoke(self, samples: List[Dict]) -> QualityScore:\n",
    "        \"\"\"Score a batch of samples\"\"\"\n",
    "\n",
    "        if not samples:\n",
    "            return QualityScore(\n",
    "                coherence=0, accuracy=0, completeness=0,\n",
    "                clarity=0, relevance=0, overall=0,\n",
    "                feedback=\"No samples to evaluate\", passed=False\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            samples_str = json.dumps(samples[:5], indent=2)  # Limit for prompt size\n",
    "            formatted_prompt = self.prompt.format(samples=samples_str)\n",
    "\n",
    "            response = self.llm.invoke(formatted_prompt)\n",
    "            response = clean_llm_output(response)\n",
    "\n",
    "            parsed = extract_json(response)\n",
    "            if parsed and isinstance(parsed, dict):\n",
    "                return QualityScore(\n",
    "                    coherence=float(parsed.get('coherence', 7)),\n",
    "                    accuracy=float(parsed.get('accuracy', 7)),\n",
    "                    completeness=float(parsed.get('completeness', 7)),\n",
    "                    clarity=float(parsed.get('clarity', 7)),\n",
    "                    relevance=float(parsed.get('relevance', 7)),\n",
    "                    overall=float(parsed.get('overall', 7)),\n",
    "                    feedback=str(parsed.get('feedback', '')),\n",
    "                    passed=float(parsed.get('overall', 7)) >= self.threshold\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Quality check warning: {e}\")\n",
    "\n",
    "        # Default passing score (optimistic)\n",
    "        return QualityScore(\n",
    "            coherence=7.5, accuracy=7.5, completeness=7.5,\n",
    "            clarity=7.5, relevance=7.5, overall=7.5,\n",
    "            feedback=\"Auto-approved\", passed=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efb4274",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetAggregator:\n",
    "    \"\"\"Collects and aggregates all generated samples\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.samples = []\n",
    "        self.quality_scores = []\n",
    "\n",
    "    def add_batch(self, samples: List[Dict], score: QualityScore):\n",
    "        \"\"\"Add a validated batch\"\"\"\n",
    "        self.samples.extend(samples)\n",
    "        self.quality_scores.append(score)\n",
    "\n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get aggregation statistics\"\"\"\n",
    "        if not self.quality_scores:\n",
    "            return {\"total\": 0}\n",
    "\n",
    "        return {\n",
    "            \"total\": len(self.samples),\n",
    "            \"avg_quality\": np.mean([s.overall for s in self.quality_scores]),\n",
    "            \"by_category\": pd.Series([s.get('category', 'unknown') for s in self.samples]).value_counts().to_dict(),\n",
    "            \"by_difficulty\": pd.Series([s.get('difficulty', 'unknown') for s in self.samples]).value_counts().to_dict()\n",
    "        }\n",
    "\n",
    "    def to_dataframe(self) -> pd.DataFrame:\n",
    "        \"\"\"Convert to pandas DataFrame\"\"\"\n",
    "        return pd.DataFrame(self.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e88533",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExporterAgent:\n",
    "    \"\"\"Exports dataset to various formats\"\"\"\n",
    "\n",
    "    def __init__(self, output_dir: str = \"/content/output\"):\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    def export_csv(self, df: pd.DataFrame, filename: str = \"dataset.csv\") -> str:\n",
    "        \"\"\"Export to CSV format\"\"\"\n",
    "        path = os.path.join(self.output_dir, filename)\n",
    "        df.to_csv(path, index=False, encoding='utf-8')\n",
    "        return path\n",
    "\n",
    "    def export_metadata(self, stats: Dict, filename: str = \"metadata.json\") -> str:\n",
    "        \"\"\"Export metadata\"\"\"\n",
    "        path = os.path.join(self.output_dir, filename)\n",
    "        metadata = {\n",
    "            \"generated_at\": datetime.now().isoformat(),\n",
    "            \"model_used\": config.MODEL_ID,\n",
    "            \"statistics\": stats,\n",
    "            \"config\": {\n",
    "                \"batch_size\": config.BATCH_SIZE,\n",
    "                \"quality_threshold\": config.MIN_QUALITY_SCORE,\n",
    "                \"temperature\": config.TEMPERATURE\n",
    "            }\n",
    "        }\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2, default=str)\n",
    "        return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fc9b13",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"Agent classes implemented\")\n",
    "print(\"   - RequirementParserAgent\")\n",
    "print(\"   - ContextBuilderAgent\")\n",
    "print(\"   - MasterDataGeneratorAgent\")\n",
    "print(\"   - QualityControllerAgent\")\n",
    "print(\"   - DatasetAggregator\")\n",
    "print(\"   - ExporterAgent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6ee83a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "============================================================================\n",
    "CELL 8: LANGGRAPH WORKFLOW\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549741d0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def create_synth_workflow(llm):\n",
    "    \"\"\"Create the LangGraph workflow for synthetic data generation\"\"\"\n",
    "\n",
    "    # Initialize agents\n",
    "    parser = RequirementParserAgent(llm)\n",
    "    context_builder = ContextBuilderAgent(llm)\n",
    "    generator = MasterDataGeneratorAgent(llm)\n",
    "    quality_controller = QualityControllerAgent(llm, threshold=config.MIN_QUALITY_SCORE)\n",
    "    aggregator = DatasetAggregator()\n",
    "    exporter = ExporterAgent(config.OUTPUT_DIR)\n",
    "\n",
    "    # Define workflow nodes - IMPORTANT: Return dict with updated keys only (LangGraph v0.2+ requirement)\n",
    "    def parse_requirements(state: AgentState) -> Dict:\n",
    "        \"\"\"Node: Parse user requirements\"\"\"\n",
    "        print(\"\\nParsing requirements...\")\n",
    "        user_input = state.get(\"user_input\", \"Generate financial Q&A dataset\")\n",
    "        requirements = parser.invoke(user_input)\n",
    "        print(f\"   Domain: {requirements.get('domain')}\")\n",
    "        print(f\"   Target: {requirements.get('target_size')} samples\")\n",
    "        # Return only the updated keys - LangGraph will merge them\n",
    "        return {\n",
    "            \"requirements\": requirements,\n",
    "            \"current_step\": \"requirements_parsed\"\n",
    "        }\n",
    "\n",
    "    def build_context(state: AgentState) -> Dict:\n",
    "        \"\"\"Node: Build domain context for current batch\"\"\"\n",
    "        categories = list(FINANCIAL_KNOWLEDGE[\"categories\"].keys())\n",
    "\n",
    "        # Get requirements from state (it's a dict, not requiring mutation)\n",
    "        requirements = state.get(\"requirements\", {})\n",
    "        \n",
    "        # Select random category and difficulty based on distribution\n",
    "        category = random.choice(categories)\n",
    "        diff_dist = requirements.get(\"difficulty_distribution\",\n",
    "                                     {\"beginner\": 0.25, \"intermediate\": 0.40,\n",
    "                                      \"advanced\": 0.25, \"expert\": 0.10})\n",
    "        difficulty = random.choices(\n",
    "            list(diff_dist.keys()),\n",
    "            weights=list(diff_dist.values())\n",
    "        )[0]\n",
    "\n",
    "        context = context_builder.invoke(category, difficulty)\n",
    "        new_context = {\n",
    "            \"category\": category,\n",
    "            \"difficulty\": difficulty,\n",
    "            **context\n",
    "        }\n",
    "        # Return only the updated keys\n",
    "        return {\n",
    "            \"context\": new_context,\n",
    "            \"current_step\": \"context_built\"\n",
    "        }\n",
    "\n",
    "    def generate_batch(state: AgentState) -> Dict:\n",
    "        \"\"\"Node: Generate a batch of Q&A pairs\"\"\"\n",
    "        context = state.get(\"context\", {})\n",
    "        category = context.get(\"category\", \"investing\")\n",
    "        difficulty = context.get(\"difficulty\", \"intermediate\")\n",
    "\n",
    "        samples = generator.invoke(\n",
    "            category=category,\n",
    "            difficulty=difficulty,\n",
    "            context=context,\n",
    "            num_samples=config.SAMPLES_PER_LLM_CALL\n",
    "        )\n",
    "\n",
    "        current_batch = state.get(\"current_batch\") or {}\n",
    "        new_batch = {\n",
    "            \"samples\": samples,\n",
    "            \"category\": category,\n",
    "            \"difficulty\": difficulty,\n",
    "            \"correction_rounds\": current_batch.get(\"correction_rounds\", 0)\n",
    "        }\n",
    "        # Return only the updated keys\n",
    "        return {\n",
    "            \"current_batch\": new_batch,\n",
    "            \"current_step\": \"batch_generated\"\n",
    "        }\n",
    "\n",
    "    def check_quality(state: AgentState) -> Dict:\n",
    "        \"\"\"Node: Quality control for current batch\"\"\"\n",
    "        batch = dict(state.get(\"current_batch\", {}))  # Make a copy\n",
    "        samples = batch.get(\"samples\", [])\n",
    "\n",
    "        if samples:\n",
    "            score = quality_controller.invoke(samples)\n",
    "            batch[\"quality_score\"] = score.model_dump()\n",
    "            batch[\"passed\"] = score.passed\n",
    "        else:\n",
    "            batch[\"passed\"] = False\n",
    "            batch[\"quality_score\"] = {\"overall\": 0, \"feedback\": \"No samples generated\"}\n",
    "\n",
    "        # Return only the updated keys\n",
    "        return {\n",
    "            \"current_batch\": batch,\n",
    "            \"current_step\": \"quality_checked\"\n",
    "        }\n",
    "\n",
    "    def should_retry(state: AgentState) -> str:\n",
    "        \"\"\"Conditional edge: Decide if batch needs retry or if we've failed too many times consecutively.\"\"\"\n",
    "        batch = state.get(\"current_batch\", {})\n",
    "        passed = batch.get(\"passed\", False)\n",
    "        rounds = batch.get(\"correction_rounds\", 0)\n",
    "        consecutive_failures = state.get(\"consecutive_failed_batches\", 0)\n",
    "\n",
    "        if passed:\n",
    "            # If passed, reset consecutive failures (will be done in next node)\n",
    "            return \"aggregate\"\n",
    "        else:\n",
    "            # If failed, check consecutive failures\n",
    "            new_consecutive = consecutive_failures + 1\n",
    "            if new_consecutive >= config.MAX_CONSECUTIVE_FAILURES:\n",
    "                return \"stop_generation\"\n",
    "            elif rounds >= config.MAX_CORRECTION_ROUNDS:\n",
    "                return \"aggregate\"\n",
    "            else:\n",
    "                return \"retry\"\n",
    "\n",
    "    def aggregate_results(state: AgentState) -> Dict:\n",
    "        \"\"\"Node: Aggregate accepted samples\"\"\"\n",
    "        batch = state.get(\"current_batch\", {})\n",
    "        samples = batch.get(\"samples\", [])\n",
    "        errors = list(state.get(\"errors\", []))  # Copy the errors list\n",
    "        \n",
    "        total_accepted = state.get(\"total_accepted\", 0)\n",
    "        consecutive_failed = state.get(\"consecutive_failed_batches\", 0)\n",
    "        hitl_pause = False\n",
    "\n",
    "        if batch.get(\"passed\", False) and samples:\n",
    "            score_dict = batch.get(\"quality_score\", {})\n",
    "            score = QualityScore(\n",
    "                coherence=score_dict.get(\"coherence\", 7.5),\n",
    "                accuracy=score_dict.get(\"accuracy\", 7.5),\n",
    "                completeness=score_dict.get(\"completeness\", 7.5),\n",
    "                clarity=score_dict.get(\"clarity\", 7.5),\n",
    "                relevance=score_dict.get(\"relevance\", 7.5),\n",
    "                overall=score_dict.get(\"overall\", 7.5),\n",
    "                feedback=str(score_dict.get(\"feedback\", \"\")),\n",
    "                passed=True\n",
    "            )\n",
    "            aggregator.add_batch(samples, score)\n",
    "            total_accepted = len(aggregator.samples)\n",
    "            consecutive_failed = 0\n",
    "        else:\n",
    "            consecutive_failed += 1\n",
    "            if batch.get(\"quality_score\"):\n",
    "                feedback = batch[\"quality_score\"].get(\"feedback\", \"\")\n",
    "                errors.append(f\"Batch failed quality check: {feedback[:50]}...\")\n",
    "            else:\n",
    "                errors.append(\"Batch failed quality check with no score.\")\n",
    "\n",
    "        total_generated = state.get(\"total_generated\", 0) + len(samples)\n",
    "\n",
    "        # Check for HITL pause\n",
    "        if config.ENABLE_HITL and total_accepted > 0:\n",
    "            if total_accepted % config.HITL_CHECKPOINT_INTERVAL == 0:\n",
    "                hitl_pause = True\n",
    "\n",
    "        # Return only the updated keys\n",
    "        return {\n",
    "            \"total_accepted\": total_accepted,\n",
    "            \"total_generated\": total_generated,\n",
    "            \"consecutive_failed_batches\": consecutive_failed,\n",
    "            \"errors\": errors,\n",
    "            \"current_step\": \"aggregated\",\n",
    "            \"hitl_pause\": hitl_pause\n",
    "        }\n",
    "\n",
    "    def should_continue(state: AgentState) -> str:\n",
    "        \"\"\"Conditional edge: Check if we need more samples or should stop due to failures.\"\"\"\n",
    "        requirements = state.get(\"requirements\", {})\n",
    "        target = requirements.get(\"target_size\", config.TARGET_SAMPLES)\n",
    "        current = state.get(\"total_accepted\", 0)\n",
    "        consecutive_failures = state.get(\"consecutive_failed_batches\", 0)\n",
    "\n",
    "        if current >= target:\n",
    "            return \"export\"\n",
    "        elif state.get(\"hitl_pause\", False):\n",
    "            return \"hitl\"\n",
    "        elif consecutive_failures >= config.MAX_CONSECUTIVE_FAILURES:\n",
    "            return \"export\"\n",
    "        else:\n",
    "            return \"continue\"\n",
    "\n",
    "\n",
    "    def hitl_checkpoint(state: AgentState) -> Dict:\n",
    "        \"\"\"Node: Human-in-the-loop checkpoint\"\"\"\n",
    "        total_accepted = state.get(\"total_accepted\", 0)\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"HITL CHECKPOINT - {total_accepted} samples generated\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(\"   Review current progress and quality scores.\")\n",
    "        print(\"   Dataset generation will continue automatically.\")\n",
    "        # Return only the updated keys\n",
    "        return {\n",
    "            \"hitl_pause\": False,\n",
    "            \"current_step\": \"hitl_complete\"\n",
    "        }\n",
    "\n",
    "    def stop_on_failure(state: AgentState) -> Dict:\n",
    "        \"\"\"Node: Handles stopping due to too many consecutive failures.\"\"\"\n",
    "        consecutive_failed = state.get('consecutive_failed_batches', 0)\n",
    "        print(f\"\\nStopping generation due to {consecutive_failed} consecutive failed batches.\")\n",
    "        # Return only the updated keys\n",
    "        return {\n",
    "            \"final_output\": \"Generation stopped due to excessive failures.\",\n",
    "            \"current_step\": \"stopped_due_to_failure\"\n",
    "        }\n",
    "\n",
    "    def export_dataset(state: AgentState) -> Dict:\n",
    "        \"\"\"Node: Export final dataset\"\"\"\n",
    "        print(\"\\nExporting dataset...\")\n",
    "\n",
    "        df = aggregator.to_dataframe()\n",
    "        stats = aggregator.get_stats()\n",
    "\n",
    "        csv_path = exporter.export_csv(df, config.CSV_FILENAME)\n",
    "        meta_path = exporter.export_metadata(stats)\n",
    "\n",
    "        print(f\"   CSV saved: {csv_path}\")\n",
    "        print(f\"   Metadata saved: {meta_path}\")\n",
    "        print(f\"   Total samples: {len(df)}\")\n",
    "\n",
    "        # Return only the updated keys\n",
    "        return {\n",
    "            \"final_output\": csv_path,\n",
    "            \"current_step\": \"exported\"\n",
    "        }\n",
    "\n",
    "\n",
    "    # Build the graph\n",
    "    workflow = StateGraph(AgentState)\n",
    "\n",
    "    # Add nodes\n",
    "    workflow.add_node(\"parse\", parse_requirements)\n",
    "    workflow.add_node(\"context\", build_context)\n",
    "    workflow.add_node(\"generate\", generate_batch)\n",
    "    workflow.add_node(\"quality\", check_quality)\n",
    "    workflow.add_node(\"aggregate\", aggregate_results)\n",
    "    workflow.add_node(\"hitl\", hitl_checkpoint)\n",
    "    workflow.add_node(\"stop_on_failure\", stop_on_failure)\n",
    "    workflow.add_node(\"export\", export_dataset)\n",
    "\n",
    "    # Add edges\n",
    "    workflow.set_entry_point(\"parse\")\n",
    "    workflow.add_edge(\"parse\", \"context\")\n",
    "    workflow.add_edge(\"context\", \"generate\")\n",
    "    workflow.add_edge(\"generate\", \"quality\")\n",
    "\n",
    "    # Quality check loop\n",
    "    workflow.add_conditional_edges(\n",
    "        \"quality\",\n",
    "        should_retry,\n",
    "        {\n",
    "            \"aggregate\": \"aggregate\",\n",
    "            \"retry\": \"context\",\n",
    "            \"stop_generation\": \"stop_on_failure\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Continue or finish loop\n",
    "    workflow.add_conditional_edges(\n",
    "        \"aggregate\",\n",
    "        should_continue,\n",
    "        {\n",
    "            \"continue\": \"context\",\n",
    "            \"hitl\": \"hitl\",\n",
    "            \"export\": \"export\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    workflow.add_edge(\"hitl\", \"context\")\n",
    "    workflow.add_edge(\"stop_on_failure\", END)\n",
    "    workflow.add_edge(\"export\", END)\n",
    "\n",
    "    # Compile workflow (no checkpointer needed for simple streaming)\n",
    "    app = workflow.compile()\n",
    "\n",
    "    return app, aggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9838d3b6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"LangGraph workflow defined\")\n",
    "print(\"   Nodes: parse -> context -> generate -> quality -> aggregate -> export\")\n",
    "print(\"   Features: quality loop, HITL checkpoints, memory persistence, consecutive failure handling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb228d82",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "============================================================================\n",
    "CELL 9: MAIN EXECUTION ENGINE\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a0e907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_generation(user_request: str = None, target_samples: int = None):\n",
    "    \"\"\"\n",
    "    Main function to run the synthetic data generation pipeline.\n",
    "\n",
    "    Args:\n",
    "        user_request: Natural language description of what to generate\n",
    "        target_samples: Number of samples to generate (default from config)\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (DataFrame, stats_dict, csv_path)\n",
    "    \"\"\"\n",
    "\n",
    "    if target_samples:\n",
    "        config.TARGET_SAMPLES = target_samples\n",
    "\n",
    "    if user_request is None:\n",
    "        user_request = f\"\"\"\n",
    "        Generate {config.TARGET_SAMPLES} high-quality financial question-answer pairs.\n",
    "        Cover all major financial categories including investing, banking, taxation,\n",
    "        retirement planning, personal finance, stock markets, and risk management.\n",
    "        Include a mix of difficulty levels from beginner to expert.\n",
    "        Ensure practical, educational content suitable for training AI models.\n",
    "        \"\"\"\n",
    "\n",
    "    console.print(Panel.fit(\n",
    "        f\"Starting SynthAgent Engine\\n\"\n",
    "        f\"   Target: {config.TARGET_SAMPLES} samples\\n\"\n",
    "        f\"   Model: {config.MODEL_ID}\\n\"\n",
    "        f\"   Quality threshold: {config.MIN_QUALITY_SCORE}/10\",\n",
    "        title=\"Generation Started\"\n",
    "    ))\n",
    "\n",
    "    # Create workflow\n",
    "    workflow, aggregator = create_synth_workflow(llm)\n",
    "\n",
    "    # Initialize state with proper default values (use empty dicts, not None)\n",
    "    initial_state = {\n",
    "        \"user_input\": user_request,\n",
    "        \"requirements\": {},\n",
    "        \"schema\": {},\n",
    "        \"context\": {},\n",
    "        \"current_batch\": {},\n",
    "        \"generated_samples\": [],\n",
    "        \"quality_scores\": [],\n",
    "        \"total_generated\": 0,\n",
    "        \"total_accepted\": 0,\n",
    "        \"errors\": [],\n",
    "        \"current_step\": \"init\",\n",
    "        \"hitl_pause\": False,\n",
    "        \"final_output\": \"\",\n",
    "        \"consecutive_failed_batches\": 0\n",
    "    }\n",
    "\n",
    "    # Run with progress tracking - set reasonable recursion limit\n",
    "    config_run = {\"recursion_limit\": 10000}\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    last_progress = 0\n",
    "\n",
    "    try:\n",
    "        # Stream execution for progress updates\n",
    "        for event in workflow.stream(initial_state, config_run):\n",
    "            # Get current state\n",
    "            current_accepted = 0\n",
    "            for node_name, node_state in event.items():\n",
    "                if isinstance(node_state, dict):\n",
    "                    current_accepted = node_state.get(\"total_accepted\", 0)\n",
    "                    current_step = node_state.get(\"current_step\", \"\")\n",
    "\n",
    "            # Progress update every 100 samples\n",
    "            if current_accepted - last_progress >= 100:\n",
    "                elapsed = (datetime.now() - start_time).total_seconds()\n",
    "                rate = current_accepted / elapsed if elapsed > 0 else 0\n",
    "                eta = (config.TARGET_SAMPLES - current_accepted) / rate if rate > 0 else 0\n",
    "\n",
    "                print(f\"   Progress: {current_accepted}/{config.TARGET_SAMPLES} \"\n",
    "                      f\"({100*current_accepted/config.TARGET_SAMPLES:.1f}%) | \"\n",
    "                      f\"Rate: {rate:.1f}/sec | ETA: {eta/60:.1f} min\")\n",
    "                last_progress = current_accepted\n",
    "\n",
    "            # Memory cleanup periodically\n",
    "            if current_accepted % 500 == 0 and current_accepted > 0:\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nGeneration interrupted by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during generation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    # Get final results\n",
    "    elapsed = (datetime.now() - start_time).total_seconds()\n",
    "    df = aggregator.to_dataframe()\n",
    "    stats = aggregator.get_stats()\n",
    "\n",
    "    # Calculate rate safely (avoid division by zero)\n",
    "    rate = len(df) / elapsed if elapsed > 0 else 0.0\n",
    "    avg_quality = stats.get('avg_quality', 0) if stats else 0\n",
    "    \n",
    "    # Display summary\n",
    "    console.print(Panel.fit(\n",
    "        f\"Generation Complete!\\n\\n\"\n",
    "        f\"Statistics:\\n\"\n",
    "        f\"   Total samples: {len(df)}\\n\"\n",
    "        f\"   Time elapsed: {elapsed/60:.1f} minutes\\n\"\n",
    "        f\"   Rate: {rate:.2f} samples/second\\n\"\n",
    "        f\"   Avg quality: {avg_quality:.2f}/10\",\n",
    "        title=\"Complete\", style=\"green\"\n",
    "    ))\n",
    "\n",
    "    # Show category distribution\n",
    "    if stats and \"by_category\" in stats and len(df) > 0:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"CATEGORY DISTRIBUTION\")\n",
    "        print(\"=\"*60)\n",
    "        for cat, count in sorted(stats[\"by_category\"].items(), key=lambda x: -x[1]):\n",
    "            pct = 100 * count / len(df) if len(df) > 0 else 0\n",
    "            bar = \"*\" * int(pct / 2)\n",
    "            print(f\"{cat:20s}: {count:5d} ({pct:5.1f}%) {bar}\")\n",
    "\n",
    "    # Show difficulty distribution\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"DIFFICULTY DISTRIBUTION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    if stats and \"by_difficulty\" in stats and len(df) > 0:\n",
    "        for diff, count in stats[\"by_difficulty\"].items():\n",
    "            pct = 100 * count / len(df) if len(df) > 0 else 0\n",
    "            bar = \"*\" * int(pct / 2)\n",
    "            print(f\"{diff:15s}: {count:5d} ({pct:5.1f}%) {bar}\")\n",
    "\n",
    "    # Export if we have samples\n",
    "    csv_path = None\n",
    "    if len(df) > 0:\n",
    "        exporter = ExporterAgent(config.OUTPUT_DIR)\n",
    "        csv_path = exporter.export_csv(df, config.CSV_FILENAME)\n",
    "        exporter.export_metadata(stats)\n",
    "        print(f\"\\nOutput saved to: {csv_path}\")\n",
    "\n",
    "    return df, stats, csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2233ac6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview_samples(df: pd.DataFrame, n: int = 5):\n",
    "    \"\"\"Preview generated samples\"\"\"\n",
    "    console.print(Panel.fit(f\"Sample Preview (showing {n} examples)\", title=\"Preview\"))\n",
    "\n",
    "    for i, row in df.head(n).iterrows():\n",
    "        print(f\"\\n{'-'*60}\")\n",
    "        print(f\"[{row.get('category', 'N/A')} | {row.get('difficulty', 'N/A')}]\")\n",
    "        print(f\"Q: {row['question']}\")\n",
    "        print(f\"A: {row['answer'][:200]}...\" if len(str(row['answer'])) > 200 else f\"A: {row['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02eb141",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "CELL 10: EXECUTION\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4a85a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Quick test (2-5 minutes)\n",
    "    df, stats, csv_path = run_generation(target_samples=10)\n",
    "\n",
    "    # Full generation (4-6 hours)\n",
    "    # df, stats, csv_path = run_generation(target_samples=10000)\n",
    "\n",
    "    # Preview results\n",
    "    preview_samples(df, n=5)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "formats": "ipynb,py",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
